{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting s3fs\n",
      "  Downloading s3fs-2024.3.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /opt/conda/lib/python3.10/site-packages (from s3fs) (2.12.1)\n",
      "Collecting fsspec==2024.3.1 (from s3fs)\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from s3fs) (3.9.3)\n",
      "Requirement already satisfied: botocore<1.34.52,>=1.34.41 in /opt/conda/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.34.51)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /opt/conda/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.16.0)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (0.11.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.0.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.34.52,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.34.52,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.9.0)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.34.52,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.26.18)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.52,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.16.0)\n",
      "Downloading s3fs-2024.3.1-py3-none-any.whl (29 kB)\n",
      "Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec, s3fs\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.6.0\n",
      "    Uninstalling fsspec-2023.6.0:\n",
      "      Successfully uninstalled fsspec-2023.6.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyter-ai 2.11.0 requires faiss-cpu, which is not installed.\n",
      "datasets 2.18.0 requires fsspec[http]<=2024.2.0,>=2023.1.0, but you have fsspec 2024.3.1 which is incompatible.\n",
      "jupyter-scheduler 2.5.1 requires fsspec==2023.6.0, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fsspec-2023.6.0 s3fs-2024.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "import numpy as np\n",
    "import io\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "Read the data **from a S3 bucket to a CSV**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>country</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>products_number</th>\n",
       "      <th>credit_card</th>\n",
       "      <th>active_member</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15634602</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15647311</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15619304</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15701354</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15737888</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  credit_score country  gender  age  tenure    balance  \\\n",
       "0     15634602           619  France  Female   42       2       0.00   \n",
       "1     15647311           608   Spain  Female   41       1   83807.86   \n",
       "2     15619304           502  France  Female   42       8  159660.80   \n",
       "3     15701354           699  France  Female   39       1       0.00   \n",
       "4     15737888           850   Spain  Female   43       2  125510.82   \n",
       "\n",
       "   products_number  credit_card  active_member  estimated_salary  churn  \n",
       "0                1            1              1         101348.88      1  \n",
       "1                1            0              1         112542.58      0  \n",
       "2                3            1              0         113931.57      1  \n",
       "3                2            0              0          93826.63      0  \n",
       "4                1            1              1          79084.10      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('s3://team-4-fp/Bank Customer Churn Prediction.csv')\n",
    "# show the first 5 rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rowes with missing values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit_score</th>\n",
       "      <th>country</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>products_number</th>\n",
       "      <th>credit_card</th>\n",
       "      <th>active_member</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   credit_score country  gender  age  tenure    balance  products_number  \\\n",
       "0           619  France  Female   42       2       0.00                1   \n",
       "1           608   Spain  Female   41       1   83807.86                1   \n",
       "2           502  France  Female   42       8  159660.80                3   \n",
       "3           699  France  Female   39       1       0.00                2   \n",
       "4           850   Spain  Female   43       2  125510.82                1   \n",
       "\n",
       "   credit_card  active_member  estimated_salary  churn  \n",
       "0            1              1         101348.88      1  \n",
       "1            0              1         112542.58      0  \n",
       "2            1              0         113931.57      1  \n",
       "3            0              0          93826.63      0  \n",
       "4            1              1          79084.10      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the columns that are not needed\n",
    "df = df.drop('customer_id', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 11 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   credit_score      10000 non-null  int64  \n",
      " 1   country           10000 non-null  object \n",
      " 2   gender            10000 non-null  object \n",
      " 3   age               10000 non-null  int64  \n",
      " 4   tenure            10000 non-null  int64  \n",
      " 5   balance           10000 non-null  float64\n",
      " 6   products_number   10000 non-null  int64  \n",
      " 7   credit_card       10000 non-null  int64  \n",
      " 8   active_member     10000 non-null  int64  \n",
      " 9   estimated_salary  10000 non-null  float64\n",
      " 10  churn             10000 non-null  int64  \n",
      "dtypes: float64(2), int64(7), object(2)\n",
      "memory usage: 859.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# show structure of the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "# Split the data into features and target label\n",
    "X = df.drop('churn', axis=1)\n",
    "y = df['churn']\n",
    "\n",
    "\n",
    "# Preprocessing for numerical columns: Filling missing values with the mean, then scaling the data\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Peprocessing for categorical columns: Filling missing values with the most frequent value then applying one-hot encoding\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Define categorical columns\n",
    "cat_cols = ['country', 'gender', 'credit_card', 'active_member']\n",
    "\n",
    "# Define numerical columns\n",
    "num_cols = ['credit_score', 'age', 'tenure', 'balance', 'products_number', 'estimated_salary']\n",
    "\n",
    "# Create ColumnTransdormer with categorical and numerical transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols),\n",
    "        ('cat', cat_transformer, cat_cols)\n",
    "        ],\n",
    "        remainder='passthrough')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Val / Test split\n",
    "\n",
    "Splitting the data into train (80%), validation (10%) and test (10%) sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the initial dataset into a training set (80% of the data) and a temporary test/validation set (20% of the data)\n",
    "train_X, temp_X, train_y, temp_y = train_test_split(X, y, train_size=0.8, random_state=1200)\n",
    "\n",
    "# Further split the temporary test/validation set into validation (50% of the temporary set, 10% of the total) and test sets (50% of the temporary set, 10% of the total)\n",
    "valid_X, test_X, valid_y, test_y = train_test_split(temp_X, temp_y, train_size=0.5, random_state=1200)\n",
    "\n",
    "# Fit and transform the training set\n",
    "train_X = preprocessor.fit_transform(train_X)\n",
    "\n",
    "# Transform the validation and test sets\n",
    "valid_X = preprocessor.transform(valid_X)\n",
    "test_X = preprocessor.transform(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack, csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming y_train, y_val, and y_test are Pandas Series, we first convert them into sparse column matrices\n",
    "train_y_sparse = csr_matrix(train_y.values.reshape(-1, 1))\n",
    "valid_y_sparse = csr_matrix(valid_y.values.reshape(-1, 1))\n",
    "test_y_sparse = csr_matrix(test_y.values.reshape(-1, 1))\n",
    "\n",
    "# Then, we combine them with the corresponding X_preprocessed sparse matrices\n",
    "train_set = hstack([train_y_sparse, train_X])\n",
    "valid_set = hstack([valid_y_sparse, valid_X])\n",
    "test_set = hstack([test_y_sparse, test_X])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the S3 upload function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "\n",
    "def upload_to_s3(matrix, bucket, filename):\n",
    "    # Transform the sparse matrix into a dense numpy array format, followed by conversion to a pandas DataFrame\n",
    "    df = pd.DataFrame(matrix.toarray())\n",
    "    \n",
    "    # Use StringIO to hold CSV data\n",
    "    placeholder = io.StringIO()\n",
    "    df.to_csv(placeholder, header=False, index=False)\n",
    "    \n",
    "    # Rewind to the beginning of the StringIO\n",
    "    placeholder.seek(0)\n",
    "    \n",
    "    # Upload csv string to S3\n",
    "    object = s3.Object(bucket, filename)\n",
    "    object.put(Body=placeholder.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining this, we proceed to the upload of the train and validation split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_to_s3(train_set, 'team-4-fp', 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_to_s3(valid_set, 'team-4-fp', 'valid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model\n",
    "\n",
    "\n",
    "We utilize the Estimator class from the sagemaker.estimator module to establish the environment for running training jobs for our model. Key configurations include:\n",
    "\n",
    "Container Name: We're leveraging a pre-existing Docker container for XGBoost, specified with sagemaker.image_uris.retrieve, indicating the algorithm version and AWS region.\n",
    "\n",
    "Role Name: The execution role fetched by sagemaker.get_execution_role(), granting necessary permissions for the training job, akin to roles used in Lambda functions.\n",
    "\n",
    "Instance Count: We're starting with a single instance (instance_count=1) for training, keeping scalability in mind for larger jobs.\n",
    "Instance Type: Selected as 'ml.m4.xlarge', a type included in the SageMaker Free Tier, to balance cost and performance.\n",
    "\n",
    "Output Path: Designated as s3://team-4-fp/sagemaker-output/, where the model artifacts and related information will be stored.\n",
    "\n",
    "Hyperparameters: Configured for a binary classification task, including parameters such as 'objective', 'eval_metric', and 'num_round', among others, to tune the XGBoost model effectively.\n",
    "\n",
    "Current Session: Utilizes sagemaker.Session(), necessary for internal management within SageMaker's environment.\n",
    "\n",
    "This setup meticulously specifies the training environment, data locations, model parameters, and AWS resources, ensuring a streamlined and efficient model training process on SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the execution role for the SageMaker session\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Determine the AWS region of the current SageMaker session\n",
    "region_name = boto3.Session().region_name\n",
    "\n",
    "# Retrieve the Docker container image for XGBoost, specifying the version and region\n",
    "container = sagemaker.image_uris.retrieve('xgboost', region_name, version='0.90-1')\n",
    "\n",
    "# Define the S3 output location where the trained model artifacts will be stored\n",
    "output_location = 's3://team-4-fp/sagemaker-output/'\n",
    "\n",
    "# Configuring XGBoost hyperparameters for binary classification:\n",
    "# - 'objective': Learning task, 'binary:logistic' for binary classification.\n",
    "# - 'eval_metric': Performance evaluation metric, 'auc' for area under curve.\n",
    "# - 'eta': Learning rate to control overfitting, set to 0.1.\n",
    "# - 'max_depth': Maximum tree depth, set to 6 for complexity control.\n",
    "# - 'min_child_weight': Minimum sum of instance weight(hessian) needed in a child.\n",
    "# - 'subsample', 'colsample_bytree': Subsampling rates for samples and features to prevent overfitting.\n",
    "# - 'gamma': Minimum loss reduction required to make further splits.\n",
    "# - 'lambda', 'alpha': L2 and L1 regularization terms on weights for model simplification and to prevent overfitting.\n",
    "hyperparams = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'num_round': '20',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'gamma': 0.1,\n",
    "    'lambda': 1,\n",
    "    'alpha': 0,\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize the Estimator object with the specified configuration\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=container,                  \n",
    "    role=role,                            \n",
    "    instance_count=1,                     \n",
    "    instance_type='ml.m4.xlarge',         \n",
    "    output_path=output_location,          \n",
    "    hyperparameters=hyperparams,          \n",
    "    sagemaker_session=sagemaker.Session() \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to crete what sagemaker calls \"channels\". We need to specify where is the data and in which format in a specific dictionary:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:The class sagemaker.session.s3_input has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "WARNING:sagemaker.deprecations:The class sagemaker.session.s3_input has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "train_channel = sagemaker.session.s3_input(\n",
    "    's3://team-4-fp/train.csv',\n",
    "    content_type='text/csv'\n",
    ")\n",
    "val_channel = sagemaker.session.s3_input(\n",
    "    's3://team-4-fp/valid.csv',\n",
    "    content_type='text/csv'\n",
    ")\n",
    "\n",
    "channels_for_training = {\n",
    "    'train': train_channel,\n",
    "    'validation': val_channel\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-04-02-13-16-28-584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2024-04-02 13:16:30 Starting - Starting the training job..\n",
      "2024-04-02 13:16:44 Starting - Preparing the instances for training..........\n",
      "2024-04-02 13:17:41 Downloading - Downloading input data.......\n",
      "2024-04-02 13:18:21 Downloading - Downloading the training image....\n",
      "2024-04-02 13:18:47 Training - Training image download completed. Training in progress....\n",
      "2024-04-02 13:19:07 Uploading - Uploading generated training model..\n",
      "2024-04-02 13:19:23 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs=channels_for_training, logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking metrics of the first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>train:auc</td>\n",
       "      <td>0.898326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>validation:auc</td>\n",
       "      <td>0.851817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp     metric_name     value\n",
       "0        0.0       train:auc  0.898326\n",
       "1        0.0  validation:auc  0.851817"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accessing the training job's performance metrics related to the set objectives and evaluations\n",
    "model_metrics = sagemaker.analytics.TrainingJobAnalytics(\n",
    "    training_job_name=estimator.latest_training_job.name,\n",
    "    # Assuming 'auc' was a key metric for your model, you might also be interested in 'logloss' and 'error' if they are relevant to your evaluation strategy.\n",
    "    metric_names=['train:auc', 'validation:auc']\n",
    ")\n",
    "\n",
    "# Displaying the metrics as a DataFrame for easy visualization and analysis\n",
    "metrics_df = model_metrics.dataframe()\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this setup, we configure and launch a hyperparameter tuning job for a binary classification model using XGBoost on Amazon SageMaker. We define a range of values for key model hyperparameters and set certain fixed parameters, including the objective and evaluation metrics. The aim is to find the optimal hyperparameter values that maximize the area under the ROC curve (AUC) on the validation dataset. We use SageMaker's hyperparameter tuning functionality to automate this process, running multiple training jobs in parallel to explore the defined hyperparameter space efficiently. The training and validation data are provided from specified S3 locations, and the tuning job's progress can be monitored through SageMaker's console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating hyperparameter tuning job with name: sagemaker-xgboost-240402-1329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................................................................!\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import sagemaker\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "hyperparameter_ranges = {\n",
    "    'eta': ContinuousParameter(0.01, 0.2),\n",
    "    'min_child_weight': ContinuousParameter(1, 10),\n",
    "    'alpha': ContinuousParameter(0, 2),\n",
    "    'max_depth': IntegerParameter(3, 10),\n",
    "    'subsample': ContinuousParameter(0.5, 1),\n",
    "    'colsample_bytree': ContinuousParameter(0.5, 1),\n",
    "    'gamma': ContinuousParameter(0, 5),\n",
    "    'lambda': ContinuousParameter(1e-5, 10),\n",
    "}\n",
    "\n",
    "# Set fixed hyperparameters and hyperparameters for early stopping\n",
    "estimator.set_hyperparameters(\n",
    "    eval_metric='auc',\n",
    "    num_round=1000,  # A high number, intending to rely on early stopping\n",
    "    objective='binary:logistic',\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "# Define the objective metric to optimize during tuning\n",
    "objective_metric_name = 'validation:auc'\n",
    "\n",
    "# Configure the hyperparameter tuner\n",
    "tuner = HyperparameterTuner(estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=30,  # Total number of training jobs\n",
    "                            max_parallel_jobs=10)  # Number of jobs to run in parallel\n",
    "\n",
    "# Specify the data channels for training and validation\n",
    "train_input = TrainingInput('s3://team-4-fp/train.csv', content_type='text/csv')\n",
    "validation_input = TrainingInput('s3://team-4-fp/valid.csv', content_type='text/csv')\n",
    "\n",
    "# Launch the hyperparameter tuning job\n",
    "tuner.fit({'train': train_input, 'validation': validation_input}, logs=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation:auc: 0.8537300229072571\n",
      "train:auc: 0.9084439873695374\n",
      "ObjectiveMetric: 0.8537300229072571\n"
     ]
    }
   ],
   "source": [
    "# Get the name of the hyperparameter tuning job\n",
    "tuning_job_name = tuner.latest_tuning_job.job_name\n",
    "\n",
    "# Retrieve the results of the tuning job\n",
    "sage_client = sagemaker.Session().sagemaker_client\n",
    "tuning_job_result = sage_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)\n",
    "\n",
    "# Get the best performing hyperparameter set\n",
    "best_hyperparameters = sage_client.list_training_jobs_for_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuning_job_name,\n",
    "    SortBy='FinalObjectiveMetricValue',\n",
    "    SortOrder='Descending',\n",
    "    MaxResults=1\n",
    ")\n",
    "\n",
    "best_job_name = best_hyperparameters['TrainingJobSummaries'][0]['TrainingJobName']\n",
    "best_job = sage_client.describe_training_job(TrainingJobName=best_job_name)\n",
    "\n",
    "# Now extract the metrics\n",
    "best_job_metrics = best_job['FinalMetricDataList']\n",
    "\n",
    "# Output the metrics\n",
    "for metric in best_job_metrics:\n",
    "    print(f\"{metric['MetricName']}: {metric['Value']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2024-04-02 13:33:58 Starting - Found matching resource for reuse\n",
      "2024-04-02 13:33:58 Downloading - Downloading the training image\n",
      "2024-04-02 13:33:58 Training - Training image download completed. Training in progress.\n",
      "2024-04-02 13:33:58 Uploading - Uploading generated training model\n",
      "2024-04-02 13:33:58 Completed - Resource reused by training job: sagemaker-xgboost-240402-1329-021-aded26cb"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2024-04-02-14-06-51-127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint-config with name sagemaker-xgboost-2024-04-02-14-06-51-127\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-xgboost-2024-04-02-14-06-51-127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "# First, get the best training job name\n",
    "best_training_job_name = tuner.best_training_job()\n",
    "\n",
    "# Then attach this best training job to a new estimator object\n",
    "best_estimator = sagemaker.estimator.Estimator.attach(best_training_job_name)\n",
    "\n",
    "# Deploy this best estimator to an endpoint\n",
    "predictor = best_estimator.deploy(initial_instance_count=1, \n",
    "                                  instance_type='ml.m4.xlarge', \n",
    "                                  serializer=sagemaker.serializers.CSVSerializer())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics of the deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84\n",
      "Recall: 0.4131455399061033\n",
      "Precision: 0.7154471544715447\n",
      "F1 Score: 0.5238095238095238\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "# Ensure predictor uses CSVSerializer to prepare the data for the endpoint\n",
    "predictor.serializer = sagemaker.serializers.CSVSerializer()\n",
    "\n",
    "# Convert your test data to CSV format\n",
    "buffer = io.StringIO()\n",
    "np.savetxt(buffer, test_X.toarray() if sparse.issparse(test_X) else test_X, delimiter=\",\", fmt='%g')\n",
    "\n",
    "# Make predictions\n",
    "predictions_csv = predictor.predict(buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "# Convert the returned CSV predictions into a list of floats\n",
    "predictions_floats = [float(prob) for prob in predictions_csv.split(',')]\n",
    "\n",
    "# Assuming your true labels (test_y) are in a format compatible with sklearn's metrics\n",
    "# Convert probabilities to binary predictions based on a 0.5 threshold\n",
    "threshold = 0.5\n",
    "binary_predictions = [1 if prob > threshold else 0 for prob in predictions_floats]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(test_y, binary_predictions)\n",
    "recall = recall_score(test_y, binary_predictions)\n",
    "precision = precision_score(test_y, binary_predictions)\n",
    "f1 = f1_score(test_y, binary_predictions)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics obtained from the model's predictions on the test dataset showcase its performance in identifying churn. The model achieves an accuracy of 84%, indicating a strong overall ability to correctly classify both churn and non-churn cases. However, when looking deeper into the metrics that provide insight into its performance on the positive class (churn), we see a mixed picture. The recall of approximately 41.31% suggests that the model identifies just over 41% of the actual churn cases, highlighting a potential area for improvement in capturing more true positives. In contrast, the precision of 71.54% is relatively high, indicating that when the model predicts churn, it is correct about 72% of the time. The F1 score, which balances precision and recall, stands at 52.38%, reflecting the need to improve recall to better the model's performance in identifying churn cases. This analysis suggests while the model is quite reliable in its predictions, further tuning or exploring different modeling approaches might enhance its ability to detect more true churn cases without significantly sacrificing precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting churn for customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.061128076165914536,0.06451965868473053,0.0803229957818985,0.17671948671340942,0.12232694029808044,0.07389398664236069,0.11089379340410233,0.6179166436195374,0.2312091737985611,0.06720710545778275\n"
     ]
    }
   ],
   "source": [
    "# Assuming subset_test_X is a small subset of your numpy array\n",
    "subset_test_X = test_X[:10] \n",
    "\n",
    "# Convert the numpy array to CSV format\n",
    "csv_buffer = io.StringIO()\n",
    "np.savetxt(csv_buffer, subset_test_X, delimiter=\",\", fmt='%g')\n",
    "\n",
    "# Reset the buffer position to the beginning\n",
    "csv_buffer.seek(0)\n",
    "\n",
    "# Use the deployed predictor to make predictions\n",
    "predictor.serializer = sagemaker.serializers.CSVSerializer()\n",
    "predictions = predictor.predict(csv_buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction results from the deployed model have provided us with the estimated probabilities of customer churn. Analyzing the output, we observe that the majority of the instances have probabilities below the typical threshold of 0.5, indicating a low likelihood of churn according to the model's current understanding. One particular prediction stands out with a probability significantly higher than 0.5, suggesting a higher risk of churn for that customer. These results are valuable as they help prioritize follow-up actions and customer retention strategies. The low probabilities indicate a well-engaged customer base, while the outlier suggests a need for targeted intervention to address potential concerns leading to churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
